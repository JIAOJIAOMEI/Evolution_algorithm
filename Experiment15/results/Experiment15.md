# Parameter combinations

At the time of TDK, we have 8 unique parameter combinations, as follows

|      | num_generations | mutation_rate | num_individuals | crossover_rate | mutation_type | crossover_type          | local_search_rate | local_search_type | search_radius | threshold |
| ---- | --------------- | ------------- | --------------- | -------------- | ------------- | ----------------------- | ----------------- | ----------------- | ------------- | --------- |
| 590  | 1000000         | 0.04          | 100             | 0.5            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    |
| 579  | 1000000         | 0.02          | 200             | 0.6            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    |
| 588  | 1000000         | 0.04          | 100             | 0.6            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    |
| 589  | 1000000         | 0.04          | 100             | 0.7            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    |
| 569  | 1000000         | 0.02          | 200             | 0.5            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    |
| 558  | 1000000         | 0.02          | 100             | 0.6            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    |
| 542  | 1000000         | 0.01          | 200             | 0.5            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    |
| 562  | 1000000         | 0.02          | 100             | 0.7            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    |

The function we tested at that time of TDK was of 50 dimensions.

Later we added two more parameters, generation gap and selection method.

```python
gg = [0.01, 0.2, 0.5, 0.8, 0.99] #5
selection_method = ["SSGA", "sorted_selection_part", "sorted_selection_all", "roulette_Wheel_Select"] #4
```

Finally, the function and algorithm parameters are added,

```python
fitness_function = [1, 5, 7, 8, 10, 13] #6
algorithm = ["Baseline", "Lamarck", "Baldwin"] #3
```

So the overall combination of parameters this time is 8X5X4X6X3=2880.(This time dim=100.)

# Recall the results of TDK

At TDK, the number of generations is the same for all three algorithms, with SSGA using F.() once and Lamarck and Baldwin using F.() twice in each generation.

The graph below is generated by me based on TDK data, with a total of 20 parameter combinations. What I want to express here is that although the quality of the final solutions of the three algorithms is more or less the same level, Baldwin and Lamarck used double the number of evaluations to reach the same level as SSGA.

![experiment15use](experiment15use.png)

The following graph is generated by combining only 8 unique parameters, again with TDK data. The chart above is TDK data with 20 parameters, which has some redundancy, and now this chart has only 8 unique parameters. The point I am trying to make here is that Baldwin and Lamarck use double the number of evaluations, but do not completely outperform SSGA, and Lamarck is sometimes even inferior to Baldwin, e.g. F8.

![experiment15use8unique](experiment15use8unique.png)

# results of experiment15

In Experiment15, the number of evaluations (budget) is the same for all three algorithms, with SSGA using F.() once and Lamarck and Baldwin using F.() twice in each generation. This means that if SSGA iterates 10 times, then Lamarck and Baldwin can only iterate 5 times.

dim=100, budget = 10,000*dim

## Crossover rate

![crossover_rate](crossover_rate.png)

## Mutation rate

![mutation_rate](mutation_rate.png)

## Generation gap

![gg](gg.png)

## Selection method

![selection_method](selection_method.png)

## Conclusion

Overall, SSGA is better than Baldwin and Baldwin is better than Lamarck. 

I think there are two reasons for this. The biggest reason is, of course, that **Baldwin and Lamarck have only half the number of iterations of baseline**, because the condition of number of evaluations has to be satisfied the same. In Experiment15, the number of evaluations (budget) is the same for all three algorithms, with SSGA using F.() once and Lamarck and Baldwin using F.() twice in each generation. 

The second reason is that the **local search procedure is not helpful for finding better solutions than genotypes**. In TDK, the number of generations is the same for all three algorithms, which fundamentally ensures that Baldwin and Lamarck produce results that are not much worse than SSGA, or at least will be SAME level if the local search procedure is not useful. If there is no better solution, then one's genotype are used as one's phenotype. This strategy, with the same number of generations and local search not producing a better solution, ensures that the quality of the final solution produced by the three algorithms is similar. 

We use **local search type: uniform; local search rate: 0.5, and local search radius: 0.1**, which make it difficult to produce better solutions than genotype. We chose this set of parameters because it can bring the multimodal functions out of the local minimas and then have the chance to find the global minimas, which is clearly the case for functions 15, 21, 22 and 23. This set of parameters can take multimodal functions out of local minimas if they are trapped there, but is meaningless for unimodal functions (1,5,7,etc). 

==Q1: why we use this local search parameter set?==

Some time before TDK, we found that the last few multimodal functions could not find the minimum value because they were stuck in local minima. If the initial position is good, it can find the global minima very quickly, but once it enters the local minima, then it will always be stuck in the local minima and cannot escape.

We all know that once the best individual in the population has found the global minima, other individuals will quickly find the global minima as well. Of course, if the best individual finds the local minima, other individuals will quickly find the local minima as well. The fact is that whatever the best individual is like, other individuals will always look to the best individual. They will adapt their genes so that they become more and more like the best individuals, otherwise they would have been eliminated. **Evolution is not just the best individuals in the population evolving, but other individuals are also evolving.** As for the direction of EVOLUTION, we define the direction of evolution, we define what is good, what is bad. This, of course, depends on the objective of our optimisation problem. **What I am trying to convey is that the similarity of populations increases with the number of iterations.**

Now there is the problem that when the population similarity is high and the best individual is stuck in the local minima, then soon the whole population will be stuck in the local minima. **When populations are stuck in the local minima, new genes are needed if they want to leave the local minima.**

There are three ways to generate new genes in a population, 1. mutation, 2. local search, and 3. crossover, both of the first two ways can only generate nearby solutions. **But nearby solutions cannot help them to get rid of the local minima, because no nearby solutions will be better than the local minima.** Crossover allows for a jump on the fitness landscape, because crossover allows parents to swap their genes over a large area. A large modification of an individual's genes can lead to a jump on the fitness landscape. **But if the parents' genes are themselves similar, then gene swapping will not produce a jump effect.**

So, in order to solve this problem that is to bring the popualtion out of the local minima, we used the parameter :**local search type: uniform; local search rate: 0.5, and local search radius: 0.1**. When the phenotype is generated based on the genotype, each gene has a probability of 0.5 to mutate. If there are 50 genes, it means that about 25 genes will change. And the local search radius determines the variation range of unifrom local search, **0.1X domain**, it is a large range. This local search paraemter set ia able to cause a "jump" on the fitness landscape.

**As long as it can get rid of the local minima, then it has the chance to find the global minima, and  finally improve the percentage of finding the global minima.** 

(**The observation that stuck in the local minima was verified.** We used Euclidean distance to measure the similarity of the entire population when stuck in lcoal minima. First we use the Euclidean distance to calculate the distance between the phenotype of any individual and the optimal individual, and then sum up these results, which is the similarity of the entire population. (it can reach 0, but the threshold=0.0001 used later) if the Euclidean similarity of the population is smaller than this threshold for some continuous iterations, then the program will terminate early, one of the termination conditions.)

==Q2:  why this local search parameter set cannot produce better solutions than genotypes?==

 let me explain why this set of parameters is difficult to produce better solutions than genotype. For example, you now have 50 dimensions of values, and then you need to change the values of 25 dimensions, because local search rate=0.5, maybe there is some progress in 13 dimensions, but there is some regression in 12 of them. In the end, the overall progress or regression is very difficult to control. **If the dimension increases, the difficulty of producing better solutions will increase greatly.**

In general, the number of evaluations of Baldwin and Lamarck is half of the baseline, and the local search procedure is meaningless for unimodal functions (such as 1, 5, 7, etc. of experiment15), so overall, SSGA is the best. 

As to why Baldwin and Lamarck are so far apart, I need to think some more about that.

==Q3: how does the dimensionality influences the performance regarding a same mutation rate?==

Also, it should be mentioned that mutation_rate has not changed, but the dimensionality has been increased from 50 to 100, which means that the difficulty of the task has been upgraded, but the ability to generate new genes has not changed, which will slow down the convergence rate to some extent.

**The reason is also very simple, assuming mutation_rate = 0.04, in the case of dimension = 50, it only needs to ensure progress in 2 dimensions to produce better solutions, but if dimension = 100, it needs to ensure progress in 4 dimensions to produce better solutions.**

# Best 20 parameter combinations

I selected the following best 20 parameter combinations by the same way we did on TDK.

Overall, **sorted_selection_part** is the best among all the selection methods.

|      | num_generations | mutation_rate | num_individuals | crossover_rate | mutation_type | crossover_type          | local_search_rate | local_search_type | search_radius | threshold | dimensions | gg   | selection_method      | algorithm |
| ---- | --------------- | ------------- | --------------- | -------------- | ------------- | ----------------------- | ----------------- | ----------------- | ------------- | --------- | ---------- | ---- | --------------------- | --------- |
| 291  | 1000000         | 0.02          | 200             | 0.5            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    | 100        | 0.99 | sorted_selection_part | Baseline  |
| 411  | 1000000         | 0.01          | 200             | 0.5            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    | 100        | 0.99 | sorted_selection_part | Baseline  |
| 351  | 1000000         | 0.02          | 100             | 0.6            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    | 100        | 0.99 | sorted_selection_part | Baseline  |
| 111  | 1000000         | 0.02          | 200             | 0.6            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    | 100        | 0.99 | sorted_selection_part | Baseline  |
| 471  | 1000000         | 0.02          | 100             | 0.7            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    | 100        | 0.99 | sorted_selection_part | Baseline  |
| 339  | 1000000         | 0.02          | 100             | 0.6            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    | 100        | 0.8  | sorted_selection_part | Baseline  |
| 51   | 1000000         | 0.04          | 100             | 0.5            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    | 100        | 0.99 | sorted_selection_part | Baseline  |
| 459  | 1000000         | 0.02          | 100             | 0.7            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    | 100        | 0.8  | sorted_selection_part | Baseline  |
| 171  | 1000000         | 0.04          | 100             | 0.6            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    | 100        | 0.99 | sorted_selection_part | Baseline  |
| 462  | 1000000         | 0.02          | 100             | 0.7            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    | 100        | 0.8  | sorted_selection_all  | Baseline  |
| 399  | 1000000         | 0.01          | 200             | 0.5            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    | 100        | 0.8  | sorted_selection_part | Baseline  |
| 231  | 1000000         | 0.04          | 100             | 0.7            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    | 100        | 0.99 | sorted_selection_part | Baseline  |
| 39   | 1000000         | 0.04          | 100             | 0.5            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    | 100        | 0.8  | sorted_selection_part | Baseline  |
| 342  | 1000000         | 0.02          | 100             | 0.6            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    | 100        | 0.8  | sorted_selection_all  | Baseline  |
| 219  | 1000000         | 0.04          | 100             | 0.7            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    | 100        | 0.8  | sorted_selection_part | Baseline  |
| 447  | 1000000         | 0.02          | 100             | 0.7            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    | 100        | 0.5  | sorted_selection_part | Baseline  |
| 159  | 1000000         | 0.04          | 100             | 0.6            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    | 100        | 0.8  | sorted_selection_part | Baseline  |
| 327  | 1000000         | 0.02          | 100             | 0.6            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    | 100        | 0.5  | sorted_selection_part | Baseline  |
| 279  | 1000000         | 0.02          | 200             | 0.5            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    | 100        | 0.8  | sorted_selection_part | Baseline  |
| 450  | 1000000         | 0.02          | 100             | 0.7            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    | 100        | 0.5  | sorted_selection_all  | Baseline  |

I also did a look at the performances of different selection methods under different generation gap.

![selection_methodcombine](selection_methodcombine.png)