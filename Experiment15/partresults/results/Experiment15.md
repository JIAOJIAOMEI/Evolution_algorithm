# Comments

The whole run is not over yet, and this current report is generated based on the results of 2394/2880 already completed. We have three algorithms, 2394/3=798, with the same parameters except for the algorithms ["baseline", "baldwin", "lamarck].

# Parameter combinations

At the time of TDK, we have 8 unique parameter combinations, as follows

|      | num_generations | mutation_rate | num_individuals | crossover_rate | mutation_type | crossover_type          | local_search_rate | local_search_type | search_radius | threshold |
| ---- | --------------- | ------------- | --------------- | -------------- | ------------- | ----------------------- | ----------------- | ----------------- | ------------- | --------- |
| 590  | 1000000         | 0.04          | 100             | 0.5            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    |
| 579  | 1000000         | 0.02          | 200             | 0.6            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    |
| 588  | 1000000         | 0.04          | 100             | 0.6            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    |
| 589  | 1000000         | 0.04          | 100             | 0.7            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    |
| 569  | 1000000         | 0.02          | 200             | 0.5            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    |
| 558  | 1000000         | 0.02          | 100             | 0.6            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    |
| 542  | 1000000         | 0.01          | 200             | 0.5            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    |
| 562  | 1000000         | 0.02          | 100             | 0.7            | Normal        | Probabilistic_crossover | 0.5               | Uniform           | 0.1           | 0.0001    |

The function we tested at that time was of 50 dimensions.

Later we added two more parameters, generation gap and selection method.

```python
gg = [0.01, 0.2, 0.5, 0.8, 0.99] #5
selection_method = ["SSGA", "sorted_selection_part", "sorted_selection_all", "roulette_Wheel_Select"] #4
```

Finally, the function and algorithm parameters are added,

```python
fitness_function = [1, 5, 7, 8, 10, 13] #6
algorithm = ["Baseline", "Lamarck", "Baldwin"] #3
```

So the overall combination of parameters this time is 8X5X4X6X3=2880.(This time dim=100.)

# Recall the results of TDK

At TDK, the number of generations is the same for all three algorithms, with SSGA using F.() once and Lamarck and Baldwin using F.() twice in each generation.

The graph below is generated by me based on TDK data, with a total of 20 parameter combinations. What I want to express here is that although the quality of the final solutions of the three algorithms is more or less the same level, Baldwin and Lamarck used double the number of evaluations to reach the same level as SSGA.

![experiment15use](experiment15use.png)

The following graph is generated by combining only 8 unique parameters, again with TDK data. The chart above is TDK data with 20 parameters, which has some redundancy, and now this chart has only 8 unique parameters. The point I am trying to make here is that Baldwin and Lamarck use double the number of evaluations, but do not completely outperform SSGA, and Lamarck is sometimes even inferior to Baldwin, e.g. F8.

![experiment15use8unique](experiment15use8unique.png)

# results of experiment15(part)

In Experiment15, the number of evaluations (budget) is the same for all three algorithms, with SSGA using F.() once and Lamarck and Baldwin using F.() twice in each generation. This means that if SSGA iterates 10 times, then Lamarck and Baldwin can only iterate 5 times.

dim=100, budget = 10,000*dim

## Crossover rate

![crossover_rate](crossover_rate.png)

## Mutation rate

![mutation_rate](mutation_rate.png)

## Generation gap

![gg](gg.png)

## Selection method

![selection_method](selection_method.png)

## Conclusion

Overall, SSGA is better than Baldwin and Baldwin is better than Lamarck. 

I think there are two reasons for this. The biggest reason is, of course, that Baldwin and Lamarck have only half the number of iterations of baseline, because the condition of number of evaluations has to be satisfied the same. 

The second reason is that in TDK, the number of generations is the same for all three algorithms, which fundamentally ensures that Baldwin and Lamarck produce results that are not much worse than SSGA, or at least will be SAME level if the local search procedure is not useful. If there is no better solution, then one's genotype are used as one's phenotype. This strategy, with the same number of generations and local search not producing a better solution, ensures that the quality of the final solution produced by the three algorithms is similar. 

We use **local search type: uniform; local search rate: 0.5, and local search radius: 0.1**, which make it difficult to produce better solutions than genotype. We chose this set of parameters because it can bring the multimodal functions out of the local minimas and then have the chance to find the global minimas, which is clearly the case for functions 15, 21, 22 and 23. 

This set of parameters can take multimodal functions out of local minimas if they are trapped there, but is meaningless for unimodal functions (1,5,7,etc).

But now the number of generations is different, because the conditions for meeting the number of evaluations are the same, plus the local search part does not offer much help, so overall, SSGA is the best. 

As to why Baldwin and Lamarck are so far apart, I need to think some more about that.

Last but not least, gg are [0.01, 0.2, 0.5, 0.8, 0.99],**the middle three parameters are better than 0.01 and 0.99.**I personally think 0.8 is the best.

selection_method are ['SSGA', 'sorted_selection_part', 'sorted_selection_all', 'roulette_Wheel_Select'], **'sorted_selection_part'>'sorted_selection_all'>'SSGA'>'roulette_Wheel_Select'**

